{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "input_files = [\"users\", \"badges\", \"tags\", \"posts\", \"posttags\", \"comments\", \"dummy\"]\n",
    "\n",
    "db_config = {\n",
    "    'host': 'localhost',\n",
    "    'database': 'ubuntu2',\n",
    "    'user': 'postgres',\n",
    "    'password': 'root',\n",
    "    'port': '5432'\n",
    "}\n",
    "\n",
    "data_directory = 'C:\\\\Users\\\\mr2714\\\\OneDrive - rit.edu\\\\Python_Projects\\\\BigData\\\\data\\\\'\n",
    "\n",
    "chunk_size = 5\n",
    "\n",
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "posts_query = \"\"\"\n",
    "    INSERT INTO posts (Id, ParentId, OwnerUserId, AcceptedAnswerId, Title, Body, Score, ViewCount, CreationDate)\n",
    "    VALUES %s\n",
    "    \"\"\"\n",
    "post_tags_query = \"\"\"\n",
    "    INSERT INTO posttags (PostId, TagId)\n",
    "    VALUES %s\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import psycopg2.extras as extras\n",
    "import os\n",
    "\n",
    "def connect():\n",
    "    return psycopg2.connect(\n",
    "        dbname=db_config['database'],\n",
    "        user=db_config['user'],\n",
    "        password=db_config['password'],\n",
    "        host=db_config['host'],\n",
    "        port=db_config['port']\n",
    "    )\n",
    "\n",
    "# method to connect and execute an sql file script for database\n",
    "def exec_sql_file(path):\n",
    "    # full_path = os.path.join(os.path.dirname(__file__), f'./{path}')\n",
    "    current_directory = os.getcwd()\n",
    "    full_path = os.path.join(current_directory, f'./{path}')\n",
    "    conn = connect()\n",
    "    cur = conn.cursor()\n",
    "    with open(full_path, 'r') as file:\n",
    "        cur.execute(file.read())\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "# method to connect and get one record from db; based on provide query\n",
    "def exec_get_one(sql, args={}):\n",
    "    conn = connect()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql, args)\n",
    "    one = cur.fetchone()\n",
    "    conn.close()\n",
    "    return one\n",
    "\n",
    "# method to connect and get all records from db; based on provide query\n",
    "def exec_get_all(sql, args={}):\n",
    "    conn = connect()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql, args)\n",
    "    # https://www.psycopg.org/docs/cursor.html#cursor.fetchall\n",
    "    list_of_tuples = cur.fetchall()\n",
    "    conn.close()\n",
    "    return list_of_tuples\n",
    "\n",
    "# method to execute a query; based on provided query\n",
    "def exec_commit(sql, args={}):\n",
    "    conn = connect()\n",
    "    cur = conn.cursor()\n",
    "    result = cur.execute(sql, args)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return result\n",
    "\n",
    "# method to excute a bulk query (i.e., to insert chunk of data togather) based on provided query and input data\n",
    "def execute_df_values(sql, tuples):\n",
    "    conn = connect()\n",
    "    cur = conn.cursor()\n",
    "    result = extras.execute_values(cur, sql, tuples)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_schema():\n",
    "    # Run the script file to create schema. \n",
    "    exec_sql_file('create_schema.sql')\n",
    "\n",
    "# create_schema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method removes entries with non-existent FK Ids from chunk_data.\n",
    "def remove_invalid_entries_links(chunk_data, valid_ids, loc):\n",
    "    # return only data that contains valid ids\n",
    "    return [data for data in chunk_data if int(data[loc]) in valid_ids]\n",
    "\n",
    "# This methods removes entries with non-existent FK Ids from chunk_data but ignore entries where the FK Id is None.\n",
    "def remove_invalid_entries_links_ignore_none(chunk_data, valid_ids, loc):\n",
    "    # return only data that contains valid ids and None ids\n",
    "    return [data for data in chunk_data if data[loc] is None or int(data[loc]) in valid_ids]\n",
    "\n",
    "# This method extract ids from chunk_data on give location to compare FK ids with existing Ids in the database.\n",
    "def extract_ids_from_chunk(chunk_data, loc):\n",
    "    # return ids extracted from given location from chunk_data\n",
    "    return [int(data[loc]) for data in chunk_data]\n",
    "\n",
    "\n",
    "# This method extract ids from chunk_data on give location to compare FK ids with existing Ids in the database. Only includes non-None values.\n",
    "def extract_ids_from_chunk_none(chunk_data, loc):\n",
    "    # # return ids extracted from given location from chunk_data only where is not None\n",
    "    return [int(data[loc]) for data in chunk_data if data[loc] is not None]\n",
    "\n",
    "# This method checks which ids are valid by querying the database and returns only valid_ids to ensure fk is not violated.\n",
    "def check_valid_fk_ids(table, ids):\n",
    "    valid_ids = set()\n",
    "    if ids:\n",
    "        # Make sure that only to check for valid table names to avoid SQL injection. \n",
    "        if table.lower() not in {t.lower() for t in input_files}:\n",
    "            raise ValueError(\"Invalid table name\")\n",
    "        sql = \"SELECT Id FROM \" + table + \" WHERE Id IN %s\"\n",
    "        result = exec_get_all(sql, (tuple(ids),))\n",
    "        valid_ids = [row[0] for row in result]\n",
    "    return valid_ids\n",
    "\n",
    "def check_valid_fk_ids_from_chunk(chunk_data, ids):\n",
    "    valid_ids = set()\n",
    "    if ids:\n",
    "        ids_set = set(ids)\n",
    "        valid_ids = {data[0] for data in chunk_data if data[0] in ids_set}\n",
    "    return valid_ids\n",
    "\n",
    "# This method removes entries with non-existent FK Ids from chunk_data.\n",
    "def remove_invalid_entries_links_chunked(chunk_data, valid_ids, loc, chunk_size=5000):\n",
    "    # return only data that contains valid ids and None ids\n",
    "    valid_ids_set = set(valid_ids)\n",
    "    filtered_data = []\n",
    "    # Process in chunks\n",
    "    for i in range(0, len(chunk_data), chunk_size):\n",
    "        chunk = chunk_data[i:i + chunk_size]\n",
    "        filtered_data.extend(data for data in chunk if int(data[loc]) in valid_ids_set)\n",
    "    \n",
    "    return filtered_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Table:  users  Record Inserted:  1450497\n",
      "Table:  badges  Record Inserted:  1977517\n",
      "Table:  tags  Record Inserted:  3155\n",
      "Table:  posts  Record Inserted:  909005\n",
      "Table:  posttags  Record Inserted:  1130897\n",
      "Table:  comments  Record Inserted:  616366\n",
      "Table:  dummy  Record Inserted:  910366\n"
     ]
    }
   ],
   "source": [
    "def report_db_statistics():\n",
    "    # loop over all the tables\n",
    "    for table in input_files:\n",
    "        # input files are known to avoid SQL injection\n",
    "        query = \"SELECT COUNT(*) FROM \" + table + \";\"\n",
    "        result = exec_get_one(query)\n",
    "        if result:\n",
    "            print(\"Table: \", table, \" Record Inserted: \", result[0])\n",
    "report_db_statistics()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def remove_self_bulk_post(chunk_data, loc):\n",
    "    # make s set of valid Ids (postIds)\n",
    "    post_ids = {data[0] for data in chunk_data}\n",
    "    # make a set of valid Ids based on location 1/3, excluding None\n",
    "    parent_ids = {data[loc] for data in chunk_data if data[loc] is not None}\n",
    "    # Find invalid Ids\n",
    "    invalid_ids = parent_ids - post_ids\n",
    "    # filter data to remove invalid Ids\n",
    "    filtered_chunk_data = [data for data in chunk_data if data[loc] not in invalid_ids]\n",
    "    return filtered_chunk_data\n",
    "\n",
    "# This method inserts data in a dummy table to ensure that correct data is entered. \n",
    "# This is done to avoid memory; as reading whole post has body and tags which can get really big. \n",
    "def dummy_posts_insert(input_file):\n",
    "    import xml.etree.ElementTree as ET\n",
    "\n",
    "    input_file = data_directory + input_file\n",
    "    context = ET.iterparse(input_file, events=(\"start\", \"end\"))\n",
    "\n",
    "    chunk_data = []\n",
    "    # read the whole data into chunk \n",
    "    for event, elem in context:\n",
    "        if elem.tag == 'row' and elem.get('Id') is not None and elem.get('OwnerUserId') is not None: \n",
    "                # Get posts data to insert into posts\n",
    "            element_data = (\n",
    "                    elem.get('Id'), elem.get('ParentId'),\n",
    "                    elem.get('OwnerUserId'), elem.get('AcceptedAnswerId')\n",
    "                )\n",
    "            chunk_data.append(element_data)\n",
    "            elem.clear()\n",
    "    # remove posts with invalid parent id\n",
    "    filter1_parents = remove_self_bulk_post(chunk_data, 1)\n",
    "    # remove posts with invlaid users\n",
    "    user_ids = extract_ids_from_chunk(filter1_parents, 2)\n",
    "    valid_user_ids = check_valid_fk_ids('users', user_ids)\n",
    "    filter_users = remove_invalid_entries_links_chunked(filter1_parents, valid_user_ids, 2)\n",
    "    # remove posts with invalid answerid\n",
    "    filter_answers = remove_self_bulk_post(filter_users, 3)\n",
    "    # delete table and recreate a dummy \n",
    "    dummy_query = \"INSERT INTO dummy (Id, ParentId, OwnerUserId, AcceptedAnswerId) VALUES %s\"\n",
    "    # insert data into dummy table \n",
    "    execute_df_values(dummy_query, filter_answers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This method to insert post tags \n",
    "# get the input file; make sure the data directory is correct and the input file is present in the data directory;\n",
    "# otherwise the execution will fail, this is applicable for all insert methods\n",
    "def insert_post_tags(post_tags_chunk, pt_query):\n",
    "    # first get all tags as they are very small to avoid excessive database connections\n",
    "    tags = exec_get_all(\"select Id, TagName from tags\")\n",
    "    # create map for easier test\n",
    "    tag_map = {tagname: tag_id for tag_id, tagname in tags}\n",
    "    processed_chunk_data = []\n",
    "    for post_id, tags_str in post_tags_chunk:\n",
    "        if tags_str is not None:\n",
    "            # clean data to get individual tags\n",
    "            tag_list = tags_str.replace('<', ' ').replace('>', ' ').split()\n",
    "            for tag in tag_list:\n",
    "                # get id for each tag\n",
    "                tag_id = tag_map.get(tag) \n",
    "                if tag_id is not None:\n",
    "                    # we already know that post id is not null; so add it to chunk data if tag id is also not null\n",
    "                    processed_chunk_data.append((int(post_id), int(tag_id)))\n",
    "    # get the FK ids from the chunck; only inserting tags where post and tag ids exist\n",
    "    post_ids = extract_ids_from_chunk(processed_chunk_data, -2)\n",
    "    # compare ids with the ids already existing in the primary table for FK ids; provide the primary table; valid_ids are returned as set\n",
    "    valid_post_ids = check_valid_fk_ids('posts', post_ids)\n",
    "    # filter out invalid ids; provide the FK index \n",
    "    valid_chunk_data = remove_invalid_entries_links(processed_chunk_data, valid_post_ids, -2)\n",
    "    # get the FK ids from the chunck; only inserting tags where post and tag ids exist\n",
    "    tag_ids = extract_ids_from_chunk(valid_chunk_data, -1)\n",
    "    # compare ids with the ids already existing in the primary table for FK ids; provide the primary table; valid_ids are returned as set\n",
    "    valid_tag_ids = check_valid_fk_ids('tags', tag_ids)\n",
    "    # get the FK ids from the chunck; provide the FK index\n",
    "    valid_chunk_data = remove_invalid_entries_links(valid_chunk_data, valid_tag_ids, -1)\n",
    "    # insert the valid data\n",
    "    execute_df_values(pt_query, valid_chunk_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FK for parent and answer id after insertion\n",
      "Processed 93 chunks for about 10000 elements.\n"
     ]
    }
   ],
   "source": [
    "def insert_posts(input_file, query, pt_query, max_chunks=5):\n",
    "    # first do a dummy for safe entry\n",
    "    dummy_posts_insert(input_file)\n",
    "    \n",
    "    input_file = data_directory + input_file\n",
    "    context = ET.iterparse(input_file, events=(\"start\", \"end\"))\n",
    "    \n",
    "    chunk_count = 0\n",
    "    elements_in_chunk = 0\n",
    "    chunk_data = []\n",
    "    post_tags_chunk = []\n",
    "\n",
    "    for event, elem in context:\n",
    "        if elem.tag == 'row':\n",
    "            if elem.get('Id') is not None and elem.get('OwnerUserId') is not None: \n",
    "                # get posts data to insert into posts\n",
    "                element_data = (\n",
    "                    elem.get('Id'),elem.get('ParentId'),\n",
    "                    elem.get('OwnerUserId'),elem.get('AcceptedAnswerId'),\n",
    "                    elem.get('Title'),elem.get('Body'),\n",
    "                    elem.get('Score'),elem.get('ViewCount'),\n",
    "                    elem.get('CreationDate')\n",
    "                )\n",
    "                # get tags data to process post tags and then insert into post tags\n",
    "                tags_data = (\n",
    "                    elem.get('Id'),\n",
    "                    elem.get('Tags')\n",
    "                )\n",
    "                chunk_data.append(element_data)\n",
    "                post_tags_chunk.append(tags_data)\n",
    "                elements_in_chunk += 1\n",
    "\n",
    "            if elements_in_chunk >= chunk_size:\n",
    "                chunk_count += 1\n",
    "                #check valid users for posts\n",
    "                user_ids = extract_ids_from_chunk(chunk_data, -7)\n",
    "                valid_user_ids = check_valid_fk_ids('users', user_ids)\n",
    "                valid_chunk_data = remove_invalid_entries_links(chunk_data, valid_user_ids, -7)\n",
    "                # check valid parents\n",
    "                parent_ids = extract_ids_from_chunk_none(valid_chunk_data, -8)\n",
    "                valid_parent_ids = check_valid_fk_ids('dummy', parent_ids)\n",
    "                valid_chunk_data = remove_invalid_entries_links_ignore_none(valid_chunk_data, valid_parent_ids, -8)\n",
    "                # check valid answers\n",
    "                answer_ids = extract_ids_from_chunk_none(valid_chunk_data, -6)\n",
    "                valid_answer_ids = check_valid_fk_ids('dummy', answer_ids)\n",
    "                valid_chunk_data = remove_invalid_entries_links_ignore_none(valid_chunk_data, valid_answer_ids, -6)\n",
    "                # insert the valid data in posts\n",
    "                execute_df_values(query, valid_chunk_data)\n",
    "                # insert post_tags ; post tags processing happens in the insert post tag method\n",
    "                insert_post_tags(post_tags_chunk, pt_query)\n",
    "                chunk_data = []\n",
    "                post_tags_chunk = []\n",
    "                elements_in_chunk = 0\n",
    "\n",
    "            elem.clear()\n",
    "\n",
    "    if elements_in_chunk > 0:\n",
    "        chunk_count += 1\n",
    "        # extract valid users ids are added\n",
    "        user_ids = extract_ids_from_chunk(chunk_data, -7)\n",
    "        valid_user_ids = check_valid_fk_ids('users', user_ids)\n",
    "        valid_chunk_data = remove_invalid_entries_links(chunk_data, valid_user_ids, -7)\n",
    "        # check valid parents\n",
    "        parent_ids = extract_ids_from_chunk_none(valid_chunk_data, -8)\n",
    "        valid_parent_ids = check_valid_fk_ids('dummy', parent_ids)\n",
    "        valid_chunk_data = remove_invalid_entries_links_ignore_none(valid_chunk_data, valid_parent_ids, -8)\n",
    "\n",
    "        # check valid answers\n",
    "        answer_ids = extract_ids_from_chunk_none(valid_chunk_data, -6)\n",
    "        valid_answer_ids = check_valid_fk_ids('dummy', answer_ids)\n",
    "        valid_chunk_data = remove_invalid_entries_links_ignore_none(valid_chunk_data, valid_answer_ids, -6)\n",
    "        # insert the valid data in posts\n",
    "        execute_df_values(query, valid_chunk_data)\n",
    "        # insert post_tags ; post tags processing happens in the insert post tag method\n",
    "        insert_post_tags(post_tags_chunk, pt_query)\n",
    "    \n",
    "    \n",
    "    print(f\"Processed {chunk_count} chunks for about {chunk_size} elements.\")\n",
    "\n",
    "create_schema()\n",
    "chunk_size = 10000\n",
    "insert_posts(\"Posts.xml\", posts_query, post_tags_query)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Creating FK for parent and answer id after insertion\n"
     ]
    },
    {
     "ename": "DuplicateObject",
     "evalue": "constraint \"fk_parentid\" for relation \"posts\" already exists\n",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mDuplicateObject\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 5\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCreating FK for parent and answer id after insertion\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m      2\u001b[0m final_query \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\"\"\u001b[39m\u001b[38;5;124mALTER TABLE Posts ADD CONSTRAINT fk_ParentId FOREIGN KEY (ParentId) REFERENCES Posts(Id) ON DELETE CASCADE; \u001b[39m\n\u001b[0;32m      3\u001b[0m \u001b[38;5;124mALTER TABLE Posts ADD CONSTRAINT fk_AcceptedAnswerId FOREIGN KEY (AcceptedAnswerId) REFERENCES Posts(Id) ON DELETE CASCADE; \u001b[39m\u001b[38;5;124m\"\"\"\u001b[39m\n\u001b[1;32m----> 5\u001b[0m \u001b[43mexec_commit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_query\u001b[49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[1;32mIn[3], line 49\u001b[0m, in \u001b[0;36mexec_commit\u001b[1;34m(sql, args)\u001b[0m\n\u001b[0;32m     47\u001b[0m conn \u001b[38;5;241m=\u001b[39m connect()\n\u001b[0;32m     48\u001b[0m cur \u001b[38;5;241m=\u001b[39m conn\u001b[38;5;241m.\u001b[39mcursor()\n\u001b[1;32m---> 49\u001b[0m result \u001b[38;5;241m=\u001b[39m \u001b[43mcur\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mexecute\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43margs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     50\u001b[0m conn\u001b[38;5;241m.\u001b[39mcommit()\n\u001b[0;32m     51\u001b[0m conn\u001b[38;5;241m.\u001b[39mclose()\n",
      "\u001b[1;31mDuplicateObject\u001b[0m: constraint \"fk_parentid\" for relation \"posts\" already exists\n"
     ]
    }
   ],
   "source": [
    "print(\"Creating FK for parent and answer id after insertion\")\n",
    "final_query = \"\"\"ALTER TABLE Posts ADD CONSTRAINT fk_ParentId FOREIGN KEY (ParentId) REFERENCES Posts(Id) ON DELETE CASCADE; \n",
    "ALTER TABLE Posts ADD CONSTRAINT fk_AcceptedAnswerId FOREIGN KEY (AcceptedAnswerId) REFERENCES Posts(Id) ON DELETE CASCADE; \"\"\"\n",
    "\n",
    "exec_commit(final_query)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "print(\"hello\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Missing postIds count: 4059\n",
      "Adding FK postid to posts\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['postgre', 'database']\n"
     ]
    }
   ],
   "source": [
    "input_string = \"<postgre><database>\"\n",
    "\n",
    "# Remove the angle brackets and split the string based on the brackets\n",
    "words = input_string.replace('<', ' ').replace('>', ' ').split()\n",
    "\n",
    "# Filter out any empty strings\n",
    "word_list = [word for word in words if word]\n",
    "\n",
    "print(word_list)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def analyze_xml(file_path):\n",
    "    key_info = {}\n",
    "    total_records = 0\n",
    "    non_empty_records = 0\n",
    "    # Required keys to check for presence and non-null values\n",
    "    required_keys = {'Id', 'OwnerUserId', 'ParentId', 'AcceptedAnswerId'}\n",
    "\n",
    "    try:\n",
    "        context = ET.iterparse(file_path, events=(\"end\",))\n",
    "        temp_unique_counts = {}\n",
    "        for event, elem in context:\n",
    "            if elem.tag == 'row':\n",
    "                total_records += 1\n",
    "                \n",
    "                # Check if all required keys are present and non-null\n",
    "                if all(key in elem.attrib and elem.attrib[key] != \"\" for key in required_keys):\n",
    "                    non_empty_records += 1\n",
    "                    for key, value in elem.attrib.items():\n",
    "                        value_length = len(value)\n",
    "                        if key not in key_info:\n",
    "                            key_info[key] = {\n",
    "                                \"max_length\": value_length,\n",
    "                                \"longest_value\": value,\n",
    "                                \"null_count\": 0, \n",
    "                                \"unique_count\": 0 \n",
    "                            }\n",
    "                            temp_unique_counts[key] = set() \n",
    "                        else:\n",
    "                            if value_length > key_info[key][\"max_length\"]:\n",
    "                                key_info[key][\"max_length\"] = value_length\n",
    "                                key_info[key][\"longest_value\"] = value\n",
    "                        # Track unique values up to a limit of 50\n",
    "                        if key_info[key][\"unique_count\"] < 50:\n",
    "                            if key not in temp_unique_counts:\n",
    "                                temp_unique_counts[key] = set()\n",
    "                            temp_unique_counts[key].add(value)\n",
    "                            key_info[key][\"unique_count\"] = min(len(temp_unique_counts[key]), 50)\n",
    "\n",
    "                # Update null counts for each key\n",
    "                for key in key_info:\n",
    "                    if key not in elem.attrib or elem.attrib[key] == \"\":\n",
    "                        key_info[key][\"null_count\"] += 1\n",
    "\n",
    "                elem.clear()\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file {file_path}: {e}\")\n",
    "\n",
    "    return key_info, total_records, non_empty_records\n",
    "\n",
    "\n",
    "\n",
    "file_path = data_directory + \"Posts\" + \".xml\"\n",
    "print(\"Exploring: \", file_path)\n",
    "key_info, total_records, non_empty_records = analyze_xml(file_path)\n",
    "print(\"total_records: \", total_records, \" total_non_empty_records: \", non_empty_records)\n",
    "for key, info in sorted(key_info.items()):\n",
    "    print(f\"Key: {key}\\n\")\n",
    "    print(f\"Max_Length: {info['max_length']}\\n\")\n",
    "    if len(info['longest_value']) > 1000:\n",
    "        print(f\"Long post with more than 1000 chars\\n\")\n",
    "    print(f\"Null count: {info['null_count']}\\n\")\n",
    "    print(f\"Unique Count: {info['unique_count']}\\n\")\n",
    "    "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
