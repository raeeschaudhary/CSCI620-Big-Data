{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "daceba4b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "print(\"hello\")\n",
    "# allowed_columns = {'Id'}\n",
    "input_files = [\"users\", \"badges\", \"tags\", \"posts\", \"posttags\", \"comments\"]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d73ae3f4",
   "metadata": {},
   "source": [
    "### Save Chunked as small data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b14df975",
   "metadata": {},
   "outputs": [],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "\n",
    "def process_xml_in_chunks(input_file, chunk_size=5, output_prefix=\"chunked_output\", max_chunks=1, xml_file=\"\"):\n",
    "    \"\"\"\n",
    "    Process large XML file in chunks of elements and stop after a certain number of chunks.\n",
    "    \n",
    "    :param input_file: Path to the large XML file.\n",
    "    :param chunk_size: Number of elements to process in each chunk.\n",
    "    :param output_prefix: Prefix for output file names.\n",
    "    :param max_chunks: Maximum number of chunks to process.\n",
    "    \"\"\"\n",
    "    context = ET.iterparse(input_file, events=(\"start\", \"end\"))\n",
    "    # _, root = next(context)  # Get the root element\n",
    "    \n",
    "    chunk_count = 0\n",
    "    elements_in_chunk = 0\n",
    "    chunk_root = ET.Element(root.tag)  # Create a new root for each chunk\n",
    "\n",
    "    for event, elem in context:\n",
    "        if event == \"end\" and elem.tag != root.tag:\n",
    "            # Add element to the current chunk\n",
    "            chunk_root.append(elem)\n",
    "            print(chunk_root)\n",
    "            elements_in_chunk += 1\n",
    "\n",
    "            # If we've reached the chunk size, save the chunk and reset\n",
    "            if elements_in_chunk >= chunk_size:\n",
    "                chunk_count += 1\n",
    "                save_chunk(chunk_root, output_prefix, chunk_count, xml_file)\n",
    "                elem.clear()\n",
    "                \n",
    "                # Clear the chunk and start over\n",
    "                chunk_root = ET.Element(root.tag)  # Create a new root for the next chunk\n",
    "                elements_in_chunk = 0\n",
    "\n",
    "            # Clear the element from memory\n",
    "\n",
    "            # Stop after processing the max number of chunks\n",
    "            if chunk_count >= max_chunks:\n",
    "                print(f\"Reached maximum chunk count ({max_chunks}). Terminating.\")\n",
    "                break\n",
    "\n",
    "    # If there are any remaining elements in the final chunk, save it\n",
    "    if elements_in_chunk > 0 and chunk_count < max_chunks:\n",
    "        chunk_count += 1\n",
    "        save_chunk(chunk_root, output_prefix, chunk_count)\n",
    "\n",
    "    print(f\"Processed {chunk_count} chunks of {chunk_size} elements each.\")\n",
    "\n",
    "def save_chunk(chunk_root, output_prefix, chunk_count, xml_file):\n",
    "    output_file = f\"small_data/{xml_file}_{output_prefix}_{chunk_count}.xml\"\n",
    "    tree = ET.ElementTree(chunk_root)\n",
    "    print(tree)\n",
    "    tree.write(output_file, encoding=\"utf-8\", xml_declaration=True)\n",
    "    print(f\"Saved chunk {chunk_count} to {output_file}\")\n",
    "\n",
    "# Usage example\n",
    "input_files = [\"Users\", \"Badges\", \"Posts\", \"PostLinks\", \"Comments\", \"PostHistory\", \"Votes\", \"Tags\"]\n",
    "for file in input_files:\n",
    "    input_file = \"askubuntu/\" + file + \".xml\"  # Path to your large XML file\n",
    "    process_xml_in_chunks(input_file, chunk_size=100, max_chunks=1, xml_file=file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "109c4167",
   "metadata": {},
   "source": [
    "### DB operation code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47d122ad",
   "metadata": {},
   "outputs": [],
   "source": [
    "# !pip install psycopg2==2.9.9\n",
    "# !pip install pyyaml==6.0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "4cef1c79",
   "metadata": {},
   "outputs": [],
   "source": [
    "import psycopg2\n",
    "import psycopg2.extras as extras\n",
    "import yaml\n",
    "import os\n",
    "\n",
    "def connect():\n",
    "    config = {}\n",
    "    # yml_path = os.path.join(os.path.dirname(__file__), './db.yml')\n",
    "    current_directory = os.getcwd()\n",
    "    yml_path = os.path.join(current_directory, 'db.yml')\n",
    "    with open(yml_path, 'r') as file:\n",
    "        config = yaml.load(file, Loader=yaml.FullLoader)\n",
    "    return psycopg2.connect(dbname=config['database'],\n",
    "                            user=config['user'],\n",
    "                            password=config['password'],\n",
    "                            host=config['host'],\n",
    "                            port=config['port'])\n",
    "\n",
    "def exec_sql_file(path):\n",
    "    # full_path = os.path.join(os.path.dirname(__file__), f'./{path}')\n",
    "    current_directory = os.getcwd()\n",
    "    full_path = os.path.join(current_directory, f'./{path}')\n",
    "    conn = connect()\n",
    "    cur = conn.cursor()\n",
    "    with open(full_path, 'r') as file:\n",
    "        cur.execute(file.read())\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "\n",
    "def exec_get_one(sql, args={}):\n",
    "    conn = connect()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql, args)\n",
    "    one = cur.fetchone()\n",
    "    conn.close()\n",
    "    return one\n",
    "\n",
    "def exec_get_all(sql, args={}):\n",
    "    conn = connect()\n",
    "    cur = conn.cursor()\n",
    "    cur.execute(sql, args)\n",
    "    # https://www.psycopg.org/docs/cursor.html#cursor.fetchall\n",
    "\n",
    "    list_of_tuples = cur.fetchall()\n",
    "    conn.close()\n",
    "    return list_of_tuples\n",
    "\n",
    "def exec_commit(sql, args={}):\n",
    "    conn = connect()\n",
    "    cur = conn.cursor()\n",
    "    result = cur.execute(sql, args)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return result\n",
    "\n",
    "def execute_df_values(sql, tuples):\n",
    "    conn = connect()\n",
    "    cur = conn.cursor()\n",
    "    result = extras.execute_values(cur, sql, tuples)\n",
    "    conn.commit()\n",
    "    conn.close()\n",
    "    return result\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd62d510",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Read and Insert data into DB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "75211a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "### ALL INSERT QUERIES\n",
    "\n",
    "chunk_size = 5000\n",
    "\n",
    "users_query = \"\"\"\n",
    "    INSERT INTO users (Id, AboutMe, AccountId, CreationDate, DisplayName,\n",
    "    DownVotes, LastAccessDate, Location, Reputation, UpVotes,\n",
    "    Views, WebsiteUrl) \n",
    "    VALUES %s\n",
    "    \"\"\"\n",
    "\n",
    "badges_query = \"\"\"\n",
    "    INSERT INTO badges (Id, Class, Date, Name, TagBased, UserId) \n",
    "    VALUES %s\n",
    "    \"\"\"\n",
    "\n",
    "posts_query = \"\"\"\n",
    "    INSERT INTO posts (Id, AcceptedAnswerId,AnswerCount,Body,\n",
    "        ClosedDate,CommentCount,CommunityOwnedDate,ContentLicense,\n",
    "        CreationDate,FavoriteCount,LastActivityDate,LastEditDate,\n",
    "        LastEditorDisplayName,LastEditorUserId,OwnerDisplayName,\n",
    "        OwnerUserId, ParentId,PostTypeId, Score,\n",
    "        Tags, Title, ViewCount) \n",
    "    VALUES %s\n",
    "    \"\"\"\n",
    "\n",
    "post_links_query = \"\"\"\n",
    "    INSERT INTO postlinks (Id, CreationDate, LinkTypeId, PostId, RelatedPostId)\n",
    "    VALUES %s\n",
    "    \"\"\"\n",
    "\n",
    "comments_query = \"\"\"\n",
    "    INSERT INTO comments (Id, ContentLicense, CreationDate, PostId, Score,\n",
    "        Text, UserDisplayName, UserId)\n",
    "    VALUES %s\n",
    "    \"\"\"\n",
    "\n",
    "post_history_query = \"\"\"\n",
    "    INSERT INTO posthistory (Id, Comment, ContentLicense, CreationDate, PostHistoryTypeId,\n",
    "        PostId, RevisionGUID, Text, UserDisplayName, UserId)\n",
    "    VALUES %s\n",
    "    \"\"\"\n",
    "\n",
    "votes_query = \"\"\"\n",
    "    INSERT INTO votes (Id, BountyAmount, CreationDate, PostId, UserId, VoteTypeId)\n",
    "    VALUES %s\n",
    "    \"\"\"\n",
    "\n",
    "tags_query = \"\"\"\n",
    "    INSERT INTO tags (Id, Count, ExcerptPostId, TagName, WikiPostId)\n",
    "    VALUES %s\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "36093503",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_foreign_key_exists(table, att_key, att_val):\n",
    "#     # SQL query to check if the userid exists in the users table\n",
    "#     if table.lower() not in {t.lower() for t in input_files} and att_key not in {c.lower() for c in allowed_columns}:\n",
    "#         raise ValueError(\"Invalid table name\")\n",
    "#     sql = \"SELECT COUNT(Id) FROM \" + table + \" WHERE \" + att_key + \" = %s\"\n",
    "#     result = exec_get_one(sql, (att_val,))\n",
    "#     return result[0] > 0\n",
    "\n",
    "def insert_chunk_into_db(query, chunk_data):\n",
    "    execute_df_values(query, chunk_data)\n",
    "    # print(f\"Inserted {len(chunk_data)} records into the database.\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe4a3a5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "### USERS INSERT\n",
    "def remove_invalid_entries(chunk_data, valid_ids, loc):\n",
    "    \"\"\"\n",
    "    Remove entries with non-existent FK Ids from chunk_data.\n",
    "    \"\"\"\n",
    "    return [data for data in chunk_data if data[loc] in valid_ids]\n",
    "\n",
    "def remove_invalid_entries_links(chunk_data, valid_ids, loc):\n",
    "    \"\"\"\n",
    "    Remove entries with non-existent FK Ids from chunk_data.\n",
    "    \"\"\"\n",
    "    return [data for data in chunk_data if int(data[loc]) in valid_ids]\n",
    "\n",
    "def extract_ids_from_chunk(chunk_data, loc):\n",
    "    \"\"\"\n",
    "    Extract Ids from chunk_data based on the location for the Id in chunk_data.\n",
    "    \"\"\"\n",
    "    return [int(data[loc]) for data in chunk_data]\n",
    "\n",
    "def check_valid_fk_ids(table, ids):\n",
    "    \"\"\"\n",
    "    Check which user_ids are valid by querying the database.\n",
    "    Returns a set of valid user_ids.\n",
    "    \"\"\"\n",
    "    valid_ids = set()\n",
    "    if ids:\n",
    "        if table.lower() not in {t.lower() for t in input_files}:\n",
    "            raise ValueError(\"Invalid table name\")\n",
    "        sql = \"SELECT Id FROM \" + table + \" WHERE Id IN %s\"\n",
    "        result = exec_get_all(sql, (tuple(ids),))\n",
    "        valid_ids = [row[0] for row in result]\n",
    "    return valid_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f928392b",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 291 chunks of 5000 elements each.\n"
     ]
    }
   ],
   "source": [
    "import xml.etree.ElementTree as ET\n",
    "exec_sql_file('create_schema.sql')\n",
    "#### INSERT INTO USERS\n",
    "\n",
    "def users_db_chunks(input_file, query, max_chunks=1):\n",
    "    \"\"\"\n",
    "    Process large XML file in chunks of elements and stop after a certain number of chunks.\n",
    "    \n",
    "    :param input_file: Path to the large XML file.\n",
    "    :param chunk_size: Number of elements to process in each chunk.\n",
    "    :param max_chunks: Maximum number of chunks to process.\n",
    "    \"\"\"\n",
    "    \n",
    "    context = ET.iterparse(input_file, events=(\"start\", \"end\"))\n",
    "    # Skip the root element but start processing children directly\n",
    "    # _, root = next(context)  # This gets the root but doesn't necessarily process it\n",
    "    \n",
    "    chunk_count = 0\n",
    "    elements_in_chunk = 0\n",
    "    chunk_data = []  # List to hold data to be inserted into the database\n",
    "\n",
    "    for event, elem in context:\n",
    "        if elem.tag == 'row':\n",
    "            # Collect data from element attributes, handle missing attributes\n",
    "            if elem.get('Id') is not None:\n",
    "                element_data = (\n",
    "                    elem.get('Id'),\n",
    "                    elem.get('AboutMe'),\n",
    "                    elem.get('AccountId'),\n",
    "                    elem.get('CreationDate'),\n",
    "                    elem.get('DisplayName'),\n",
    "                    elem.get('DownVotes'),\n",
    "                    elem.get('LastAccessDate'),\n",
    "                    elem.get('Location'),\n",
    "                    elem.get('Reputation'),\n",
    "                    elem.get('UpVotes'),\n",
    "                    elem.get('Views'),\n",
    "                    elem.get('WebsiteUrl')\n",
    "                )\n",
    "                chunk_data.append(element_data)\n",
    "                elements_in_chunk += 1\n",
    "\n",
    "            # If we've reached the chunk size, save the chunk and reset\n",
    "            if elements_in_chunk >= chunk_size:\n",
    "                chunk_count += 1\n",
    "                execute_df_values(users_query, chunk_data)\n",
    "                chunk_data = []  # Reset chunk data\n",
    "                elements_in_chunk = 0\n",
    "\n",
    "            elem.clear()\n",
    "\n",
    "            # Stop after processing the max number of chunks # Remove this code\n",
    "            # if max_chunks > 0 and chunk_count >= max_chunks:\n",
    "            #     print(f\"Reached maximum chunk count ({max_chunks}). Terminating.\")\n",
    "            #     break\n",
    "\n",
    "    # If there are any remaining elements in the final chunk, save it\n",
    "    if elements_in_chunk > 0:\n",
    "        chunk_count += 1\n",
    "        execute_df_values(query, chunk_data)\n",
    "\n",
    "    print(f\"Processed {chunk_count} chunks of {chunk_size} elements each.\")\n",
    "\n",
    "for file in [\"Users\"]:\n",
    "    input_file = \"askubuntu/\" + file + \".xml\"  # Path to your large XML file\n",
    "    users_db_chunks(input_file, users_query, max_chunks=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77fb1a8d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def check_foreign_keys_in_chunk(table, ids):\n",
    "#     # Create a query to check the existence of multiple IDs\n",
    "#     formatted_ids = ','.join(map(str, ids))\n",
    "#     sql = f\"SELECT COUNT(*) FROM {table} WHERE Id IN ({formatted_ids})\"\n",
    "#     result = exec_get_one(sql)\n",
    "#     return result[0]\n",
    "\n",
    "# def check_foreign_key_exists(table, att_key, att_val):\n",
    "#     # SQL query to check if the userid exists in the users table\n",
    "#     if table.lower() not in {t.lower() for t in input_files} and att_key not in {c.lower() for c in allowed_columns}:\n",
    "#         raise ValueError(\"Invalid table name\")\n",
    "#     sql = \"SELECT COUNT(Id) FROM \" + table + \" WHERE \" + att_key + \" = %s\"\n",
    "#     result = exec_get_one(sql, (att_val,))\n",
    "#     return result[0] > 0\n",
    "\n",
    "\n",
    "\n",
    "def remove_invalid_entries(chunk_data, valid_ids, loc):\n",
    "    \"\"\"\n",
    "    Remove entries with non-existent FK Ids from chunk_data.\n",
    "    \"\"\"\n",
    "    return [data for data in chunk_data if data[loc] in valid_ids]\n",
    "\n",
    "def remove_invalid_entries_links(chunk_data, valid_ids, loc):\n",
    "    \"\"\"\n",
    "    Remove entries with non-existent FK Ids from chunk_data.\n",
    "    \"\"\"\n",
    "    return [data for data in chunk_data if int(data[loc]) in valid_ids]\n",
    "\n",
    "def extract_ids_from_chunk(chunk_data, loc):\n",
    "    \"\"\"\n",
    "    Extract Ids from chunk_data based on the location for the Id in chunk_data.\n",
    "    \"\"\"\n",
    "    return [int(data[loc]) for data in chunk_data]\n",
    "\n",
    "def check_valid_fk_ids(table, ids):\n",
    "    \"\"\"\n",
    "    Check which user_ids are valid by querying the database.\n",
    "    Returns a set of valid user_ids.\n",
    "    \"\"\"\n",
    "    valid_ids = set()\n",
    "    if ids:\n",
    "        if table.lower() not in {t.lower() for t in input_files}:\n",
    "            raise ValueError(\"Invalid table name\")\n",
    "        sql = \"SELECT Id FROM \" + table + \" WHERE Id IN %s\"\n",
    "        result = exec_get_all(sql, (tuple(ids),))\n",
    "        valid_ids = [row[0] for row in result]\n",
    "    return valid_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f966615a",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Badges Insert"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "146134d6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 396 chunks of 5000 elements each.\n"
     ]
    }
   ],
   "source": [
    "#### INSERT INTO BADGES\n",
    "exec_sql_file('create_schema.sql')\n",
    "\n",
    "def badges_db_chunks(input_file, query, max_chunks=1):\n",
    "    \"\"\"\n",
    "    Process large XML file in chunks of elements and stop after a certain number of chunks.\n",
    "    \n",
    "    :param input_file: Path to the large XML file.\n",
    "    :param max_chunks: Maximum number of chunks to process. MUST REMOVE\n",
    "    \"\"\"\n",
    "    \n",
    "    context = ET.iterparse(input_file, events=(\"start\", \"end\"))\n",
    "    \n",
    "    # Skip the root element but start processing children directly\n",
    "    # _, root = next(context)  # This gets the root but doesn't necessarily process it\n",
    "    \n",
    "    chunk_count = 0\n",
    "    elements_in_chunk = 0\n",
    "    chunk_data = []  # List to hold data to be inserted into the database\n",
    "    for event, elem in context:\n",
    "        if elem.tag == 'row':\n",
    "            # Collect data from element attributes, handle missing attributes\n",
    "            if elem.get('UserId') is not None:\n",
    "                element_data = (\n",
    "                        elem.get('Id'),\n",
    "                        elem.get('Class'),\n",
    "                        elem.get('Date'),\n",
    "                        elem.get('Name'),\n",
    "                        elem.get('TagBased'),\n",
    "                        elem.get('UserId')\n",
    "                )\n",
    "                chunk_data.append(element_data)\n",
    "                elements_in_chunk += 1\n",
    "\n",
    "            # If we've reached the chunk size, save the chunk and reset\n",
    "            if elements_in_chunk >= chunk_size:\n",
    "                chunk_count += 1\n",
    "                user_ids = extract_ids_from_chunk(chunk_data, -1)\n",
    "                valid_user_ids = check_valid_fk_ids('users', user_ids)\n",
    "                if len(valid_user_ids) == len(set(user_ids)):\n",
    "                    # All UserIds are valid, proceed with bulk insertion\n",
    "                    execute_df_values(query, chunk_data)\n",
    "                    chunk_data = []  # Reset chunk data\n",
    "                    elements_in_chunk = 0\n",
    "                else:\n",
    "                    # Some UserIds are invalid, filter out invalid entries\n",
    "                    valid_chunk_data = remove_invalid_entries_links(chunk_data, valid_user_ids, -1)\n",
    "                    execute_df_values(query, valid_chunk_data)\n",
    "                    chunk_data = []  # Reset chunk data\n",
    "                    elements_in_chunk = 0\n",
    "                        \n",
    "            elem.clear()\n",
    "\n",
    "            # Stop after processing the max number of chunks\n",
    "#             if chunk_count >= max_chunks:\n",
    "#                 print(f\"Reached maximum chunk count ({max_chunks}). Terminating.\")\n",
    "#                 break\n",
    "\n",
    "    # If there are any remaining elements in the final chunk, save it\n",
    "    if elements_in_chunk > 0: # and chunk_count < max_chunks:\n",
    "        chunk_count += 1\n",
    "        user_ids = extract_ids_from_chunk(chunk_data, -1)\n",
    "        valid_user_ids = check_valid_fk_ids('users', user_ids)\n",
    "        if len(valid_user_ids) == len(set(user_ids)):\n",
    "            # All UserIds are valid, proceed with bulk insertion\n",
    "            execute_df_values(query, chunk_data)\n",
    "        else:\n",
    "            # Some UserIds are invalid, filter out invalid entries\n",
    "            valid_chunk_data = remove_invalid_entries_links(chunk_data, valid_user_ids, -1)\n",
    "            execute_df_values(query, valid_chunk_data)\n",
    "\n",
    "    print(f\"Processed {chunk_count} chunks of {chunk_size} elements each.\")\n",
    "\n",
    "for file in [\"Badges\"]:\n",
    "    input_file = \"askubuntu/\" + file + \".xml\"  # Path to your large XML file\n",
    "    badges_db_chunks(input_file, badges_query, max_chunks=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db801fa8",
   "metadata": {},
   "outputs": [],
   "source": [
    "### POSTS INSERTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "410ed9e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INSERT INTO POSTS\n",
    "exec_sql_file('create_schema.sql')\n",
    "\n",
    "def posts_db_chunks(input_file, query, max_chunks=1):\n",
    "    \"\"\"\n",
    "    Process large XML file in chunks of elements and stop after a certain number of chunks.\n",
    "    \n",
    "    :param input_file: Path to the large XML file.\n",
    "    :param chunk_size: Number of elements to process in each chunk.\n",
    "    :param max_chunks: Maximum number of chunks to process. MUST REMOVE\n",
    "    \"\"\"\n",
    "    \n",
    "    context = ET.iterparse(input_file, events=(\"start\", \"end\"))\n",
    "    \n",
    "    # Skip the root element but start processing children directly\n",
    "    # _, root = next(context)  # This gets the root but doesn't necessarily process it\n",
    "    \n",
    "    chunk_count = 0\n",
    "    elements_in_chunk = 0\n",
    "    chunk_data = []  # List to hold data to be inserted into the database\n",
    "\n",
    "    for event, elem in context:\n",
    "        if elem.tag == 'row':\n",
    "            # Collect data from element attributes, handle missing attributes\n",
    "            if elem.get('Id') is not None and elem.get('OwnerUserId') is not None:\n",
    "                element_data = (\n",
    "                    elem.get('Id'),elem.get('AcceptedAnswerId'),elem.get('AnswerCount'),\n",
    "                    elem.get('Body'),elem.get('ClosedDate'),elem.get('CommentCount'),\n",
    "                    elem.get('CommunityOwnedDate'),elem.get('ContentLicense'),\n",
    "                    elem.get('CreationDate'),elem.get('FavoriteCount'),\n",
    "                    elem.get('LastActivityDate'),elem.get('LastEditDate'),\n",
    "                    elem.get('LastEditorDisplayName'),\n",
    "                    elem.get('LastEditorUserId'),\n",
    "                    elem.get('OwnerDisplayName'),\n",
    "                    elem.get('OwnerUserId'),elem.get('ParentId'),\n",
    "                    elem.get('PostTypeId'),elem.get('Score'),\n",
    "                    elem.get('Score'),elem.get('Title'),\n",
    "                    elem.get('ViewCount')\n",
    "                )\n",
    "                chunk_data.append(element_data)\n",
    "                elements_in_chunk += 1\n",
    "\n",
    "            # If we've reached the chunk size, save the chunk and reset\n",
    "            if elements_in_chunk >= chunk_size:\n",
    "                chunk_count += 1\n",
    "                print(chunk_data)\n",
    "                user_ids = extract_ids_from_chunk(chunk_data, -7)\n",
    "                valid_user_ids = check_valid_fk_ids('users', user_ids)\n",
    "                if len(valid_user_ids) == len(set(user_ids)):\n",
    "                    # All UserIds are valid, proceed with bulk insertion\n",
    "                    execute_df_values(query, chunk_data)\n",
    "                    chunk_data = []  # Reset chunk data\n",
    "                    elements_in_chunk = 0\n",
    "                else:\n",
    "                    # Some UserIds are invalid, filter out invalid entries\n",
    "                    valid_chunk_data = remove_invalid_entries_links(chunk_data, valid_user_ids, -7)\n",
    "                    execute_df_values(query, valid_chunk_data)\n",
    "                    chunk_data = []  # Reset chunk data\n",
    "                    elements_in_chunk = 0\n",
    "\n",
    "            elem.clear()\n",
    "\n",
    "            # Stop after processing the max number of chunks\n",
    "#             if chunk_count >= max_chunks:\n",
    "#                 print(f\"Reached maximum chunk count ({max_chunks}). Terminating.\")\n",
    "#                 break\n",
    "\n",
    "    # If there are any remaining elements in the final chunk, save it\n",
    "    if elements_in_chunk > 0: # and chunk_count < max_chunks:\n",
    "        chunk_count += 1\n",
    "        user_ids = extract_ids_from_chunk(chunk_data, -7)\n",
    "        valid_user_ids = check_valid_fk_ids('users', user_ids)\n",
    "        if len(valid_user_ids) == len(set(user_ids)):\n",
    "            # All UserIds are valid, proceed with bulk insertion\n",
    "            execute_df_values(query, chunk_data)\n",
    "        else:\n",
    "            # Some UserIds are invalid, filter out invalid entries\n",
    "            valid_chunk_data = remove_invalid_entries_links(chunk_data, valid_user_ids, -7)\n",
    "            execute_df_values(query, valid_chunk_data)\n",
    "\n",
    "    print(f\"Processed {chunk_count} chunks of {chunk_size} elements each.\")\n",
    "\n",
    "for file in [\"Posts\"]:\n",
    "    input_file = \"askubuntu/\" + file + \".xml\"  # Path to your large XML file\n",
    "    posts_db_chunks(input_file, posts_query, max_chunks=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e0bc81c",
   "metadata": {},
   "outputs": [],
   "source": [
    "### POSTLINKS INSERT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "40d9a6b6",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INSERT INTO POSTLINKS\n",
    "exec_sql_file('create_schema.sql')\n",
    "\n",
    "def post_links_db_chunks(input_file, query, max_chunks=1):\n",
    "    \"\"\"\n",
    "    Process large XML file in chunks of elements and stop after a certain number of chunks.\n",
    "    \n",
    "    :param input_file: Path to the large XML file.\n",
    "    :param chunk_size: Number of elements to process in each chunk.\n",
    "    :param max_chunks: Maximum number of chunks to process. MUST REMOVE\n",
    "    \"\"\"\n",
    "    \n",
    "    context = ET.iterparse(input_file, events=(\"start\", \"end\"))\n",
    "    \n",
    "    # Skip the root element but start processing children directly\n",
    "    # _, root = next(context)  # This gets the root but doesn't necessarily process it\n",
    "    \n",
    "    chunk_count = 0\n",
    "    elements_in_chunk = 0\n",
    "    chunk_data = []  # List to hold data to be inserted into the database\n",
    "\n",
    "    for event, elem in context:\n",
    "        if elem.tag == 'row':\n",
    "            # Collect data from element attributes, handle missing attributes\n",
    "            if elem.get('Id') is not None and elem.get('PostId') is not None and elem.get('RelatedPostId') is not None:\n",
    "                element_data = (\n",
    "                    elem.get('Id'),elem.get('CreationDate'),\n",
    "                    elem.get('LinkTypeId'),elem.get('PostId'),\n",
    "                    elem.get('RelatedPostId')\n",
    "                )\n",
    "                chunk_data.append(element_data)\n",
    "                elements_in_chunk += 1\n",
    "\n",
    "            # If we've reached the chunk size, save the chunk and reset\n",
    "            if elements_in_chunk >= chunk_size:\n",
    "                chunk_count += 1\n",
    "                post_ids = extract_ids_from_chunk(chunk_data, -2)\n",
    "                valid_post_ids = check_valid_fk_ids('posts', post_ids)\n",
    "                related_post_ids = extract_ids_from_chunk(chunk_data, -1)\n",
    "                valid_related_post_ids = check_valid_fk_ids('posts', related_post_ids)\n",
    "                \n",
    "                if len(valid_post_ids) == len(set(post_ids)) and len(valid_related_post_ids) == len(set(related_post_ids)):\n",
    "                    # All UserIds are valid, proceed with bulk insertion\n",
    "                    insert_chunk_into_db(query, chunk_data)\n",
    "                    chunk_data = []  # Reset chunk data\n",
    "                    elements_in_chunk = 0\n",
    "                else:\n",
    "                    # Some UserIds are invalid, filter out invalid entries\n",
    "#                     valid_chunk_data = remove_invalid_entries(chunk_data, valid_post_ids, -2)\n",
    "                    valid_chunk_data = remove_invalid_entries_links(chunk_data, valid_post_ids, -2)\n",
    "                    valid_chunk_data = remove_invalid_entries_links(valid_chunk_data, valid_related_post_ids, -1)\n",
    "                    insert_chunk_into_db(query, valid_chunk_data)\n",
    "                    chunk_data = []  # Reset chunk data\n",
    "                    elements_in_chunk = 0\n",
    "\n",
    "            elem.clear()\n",
    "\n",
    "            # Stop after processing the max number of chunks\n",
    "#             if chunk_count >= max_chunks:\n",
    "#                 print(f\"Reached maximum chunk count ({max_chunks}). Terminating.\")\n",
    "#                 break\n",
    "\n",
    "    # If there are any remaining elements in the final chunk, save it\n",
    "    if elements_in_chunk > 0: # and chunk_count < max_chunks:\n",
    "        chunk_count += 1\n",
    "        post_ids = extract_ids_from_chunk(chunk_data, -2)\n",
    "        valid_post_ids = check_valid_fk_ids('posts', post_ids)\n",
    "        related_post_ids = extract_ids_from_chunk(chunk_data, -1)\n",
    "        valid_related_post_ids = check_valid_fk_ids('posts', related_post_ids)\n",
    "        if len(valid_post_ids) == len(set(post_ids)) and len(valid_related_post_ids) == len(set(related_post_ids)):\n",
    "            # All UserIds are valid, proceed with bulk insertion\n",
    "            insert_chunk_into_db(query, chunk_data)\n",
    "        else:\n",
    "            # Some UserIds are invalid, filter out invalid entries\n",
    "#             valid_chunk_data = remove_invalid_entries(chunk_data, valid_post_ids)\n",
    "            valid_chunk_data = remove_invalid_entries_links(chunk_data, valid_post_ids, -2)\n",
    "            valid_chunk_data = remove_invalid_entries_links(valid_chunk_data, valid_related_post_ids, -1)\n",
    "            insert_chunk_into_db(query, valid_chunk_data)\n",
    "\n",
    "    print(f\"Processed {chunk_count} chunks of {chunk_size} elements each.\")\n",
    "\n",
    "for file in [\"PostLinks\"]:\n",
    "    input_file = \"askubuntu/\" + file + \".xml\"  # Path to your large XML file\n",
    "    post_links_db_chunks(input_file, post_links_query, max_chunks=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25c2581e",
   "metadata": {},
   "outputs": [],
   "source": [
    "### INSERT INTO COMMENTS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a8dbbb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INSERT INTO COMMENTS\n",
    "exec_sql_file('create_schema.sql')\n",
    "\n",
    "def comments_db_chunks(input_file, query, max_chunks=1):\n",
    "    \"\"\"\n",
    "    Process large XML file in chunks of elements and stop after a certain number of chunks.\n",
    "    \n",
    "    :param input_file: Path to the large XML file.\n",
    "    :param chunk_size: Number of elements to process in each chunk.\n",
    "    :param max_chunks: Maximum number of chunks to process. MUST REMOVE\n",
    "    \"\"\"\n",
    "    \n",
    "    context = ET.iterparse(input_file, events=(\"start\", \"end\"))\n",
    "    \n",
    "    # Skip the root element but start processing children directly\n",
    "    # _, root = next(context)  # This gets the root but doesn't necessarily process it\n",
    "    \n",
    "    chunk_count = 0\n",
    "    elements_in_chunk = 0\n",
    "    chunk_data = []  # List to hold data to be inserted into the database\n",
    "\n",
    "    for event, elem in context:\n",
    "        if elem.tag == 'row':\n",
    "            # Collect data from element attributes, handle missing attributes\n",
    "            if elem.get('Id') is not None and elem.get('PostId') is not None and elem.get('UserId') is not None:\n",
    "                element_data = (\n",
    "                    elem.get('Id'),elem.get('ContentLicense'),\n",
    "                    elem.get('CreationDate'),elem.get('PostId'),\n",
    "                    elem.get('Score'),elem.get('Text'),\n",
    "                    elem.get('UserDisplayName'),elem.get('UserId')\n",
    "                )\n",
    "                chunk_data.append(element_data)\n",
    "                elements_in_chunk += 1\n",
    "\n",
    "            # If we've reached the chunk size, save the chunk and reset\n",
    "            if elements_in_chunk >= chunk_size:\n",
    "                chunk_count += 1\n",
    "                post_ids = extract_ids_from_chunk(chunk_data, -5)\n",
    "                valid_post_ids = check_valid_fk_ids('posts', post_ids)\n",
    "                user_ids = extract_ids_from_chunk(chunk_data, -1)\n",
    "                valid_user_ids = check_valid_fk_ids('users', user_ids)\n",
    "                \n",
    "                if len(valid_post_ids) == len(set(post_ids)) and len(user_ids) == len(set(valid_user_ids)):\n",
    "                    # All UserIds are valid, proceed with bulk insertion\n",
    "                    insert_chunk_into_db(query, chunk_data)\n",
    "                    chunk_data = []  # Reset chunk data\n",
    "                    elements_in_chunk = 0\n",
    "                else:\n",
    "                    # Some UserIds are invalid, filter out invalid entries\n",
    "#                     valid_chunk_data = remove_invalid_entries(chunk_data, valid_post_ids, -2)\n",
    "                    valid_chunk_data = remove_invalid_entries_links(chunk_data, valid_post_ids, -5)\n",
    "                    valid_chunk_data = remove_invalid_entries_links(valid_chunk_data, valid_user_ids, -1)\n",
    "                    insert_chunk_into_db(query, valid_chunk_data)\n",
    "                    chunk_data = []  # Reset chunk data\n",
    "                    elements_in_chunk = 0\n",
    "\n",
    "            elem.clear()\n",
    "\n",
    "            # Stop after processing the max number of chunks\n",
    "#             if chunk_count >= max_chunks:\n",
    "#                 print(f\"Reached maximum chunk count ({max_chunks}). Terminating.\")\n",
    "#                 break\n",
    "\n",
    "    # If there are any remaining elements in the final chunk, save it\n",
    "    if elements_in_chunk > 0: # and chunk_count < max_chunks:\n",
    "        chunk_count += 1\n",
    "        post_ids = extract_ids_from_chunk(chunk_data, -5)\n",
    "        valid_post_ids = check_valid_fk_ids('posts', post_ids)\n",
    "        user_ids = extract_ids_from_chunk(chunk_data, -1)\n",
    "        valid_user_ids = check_valid_fk_ids('users', user_ids)\n",
    "        if len(valid_post_ids) == len(set(post_ids)) and len(valid_user_ids) == len(set(user_ids)):\n",
    "            # All UserIds are valid, proceed with bulk insertion\n",
    "            insert_chunk_into_db(query, chunk_data)\n",
    "        else:\n",
    "            # Some UserIds are invalid, filter out invalid entries\n",
    "#             valid_chunk_data = remove_invalid_entries(chunk_data, valid_post_ids)\n",
    "            valid_chunk_data = remove_invalid_entries_links(chunk_data, valid_post_ids, -5)\n",
    "            valid_chunk_data = remove_invalid_entries_links(valid_chunk_data, valid_user_ids, -1)\n",
    "            insert_chunk_into_db(query, valid_chunk_data)\n",
    "\n",
    "    print(f\"Processed {chunk_count} chunks of {chunk_size} elements each.\")\n",
    "\n",
    "\n",
    "for file in [\"Comments\"]:\n",
    "    input_file = \"askubuntu/\" + file + \".xml\"  # Path to your large XML file\n",
    "    comments_db_chunks(input_file, comments_query, max_chunks=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39a218bd",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INSERT INTO POST HISTORY\n",
    "exec_sql_file('create_schema.sql')\n",
    "\n",
    "def post_hist_db_chunks(input_file, query, max_chunks=1):\n",
    "    \"\"\"\n",
    "    Process large XML file in chunks of elements and stop after a certain number of chunks.\n",
    "    \n",
    "    :param input_file: Path to the large XML file.\n",
    "    :param chunk_size: Number of elements to process in each chunk.\n",
    "    :param max_chunks: Maximum number of chunks to process. MUST REMOVE\n",
    "    \"\"\"\n",
    "    \n",
    "    context = ET.iterparse(input_file, events=(\"start\", \"end\"))\n",
    "    \n",
    "    # Skip the root element but start processing children directly\n",
    "    # _, root = next(context)  # This gets the root but doesn't necessarily process it\n",
    "    \n",
    "    chunk_count = 0\n",
    "    elements_in_chunk = 0\n",
    "    chunk_data = []  # List to hold data to be inserted into the database\n",
    "\n",
    "    for event, elem in context:\n",
    "        if elem.tag == 'row':\n",
    "            # Collect data from element attributes, handle missing attributes\n",
    "            if elem.get('Id') is not None and elem.get('PostId') is not None and elem.get('UserId') is not None:\n",
    "                element_data = (\n",
    "                    elem.get('Id'),elem.get('Comment'),\n",
    "                    elem.get('ContentLicense'),elem.get('CreationDate'),\n",
    "                    elem.get('PostHistoryTypeId'),\n",
    "                    elem.get('PostId'),elem.get('RevisionGUID'),\n",
    "                    elem.get('Text'),elem.get('UserDisplayName'),\n",
    "                    elem.get('UserId')\n",
    "                )\n",
    "                chunk_data.append(element_data)\n",
    "                elements_in_chunk += 1\n",
    "\n",
    "            # If we've reached the chunk size, save the chunk and reset\n",
    "            if elements_in_chunk >= chunk_size:\n",
    "                chunk_count += 1\n",
    "                post_ids = extract_ids_from_chunk(chunk_data, -5)\n",
    "                valid_post_ids = check_valid_fk_ids('posts', post_ids)\n",
    "                user_ids = extract_ids_from_chunk(chunk_data, -1)\n",
    "                valid_user_ids = check_valid_fk_ids('users', user_ids)\n",
    "                \n",
    "                if len(valid_post_ids) == len(set(post_ids)) and len(user_ids) == len(set(valid_user_ids)):\n",
    "                    # All UserIds are valid, proceed with bulk insertion\n",
    "                    insert_chunk_into_db(query, chunk_data)\n",
    "                    chunk_data = []  # Reset chunk data\n",
    "                    elements_in_chunk = 0\n",
    "                else:\n",
    "                    # Some UserIds are invalid, filter out invalid entries\n",
    "#                     valid_chunk_data = remove_invalid_entries(chunk_data, valid_post_ids, -2)\n",
    "                    valid_chunk_data = remove_invalid_entries_links(chunk_data, valid_post_ids, -5)\n",
    "                    valid_chunk_data = remove_invalid_entries_links(valid_chunk_data, valid_user_ids, -1)\n",
    "                    insert_chunk_into_db(query, valid_chunk_data)\n",
    "                    chunk_data = []  # Reset chunk data\n",
    "                    elements_in_chunk = 0\n",
    "\n",
    "            elem.clear()\n",
    "\n",
    "            # Stop after processing the max number of chunks\n",
    "#             if chunk_count >= max_chunks:\n",
    "#                 print(f\"Reached maximum chunk count ({max_chunks}). Terminating.\")\n",
    "#                 break\n",
    "\n",
    "    # If there are any remaining elements in the final chunk, save it\n",
    "    if elements_in_chunk > 0: # and chunk_count < max_chunks:\n",
    "        chunk_count += 1\n",
    "        post_ids = extract_ids_from_chunk(chunk_data, -5)\n",
    "        valid_post_ids = check_valid_fk_ids('posts', post_ids)\n",
    "        user_ids = extract_ids_from_chunk(chunk_data, -1)\n",
    "        valid_user_ids = check_valid_fk_ids('users', user_ids)\n",
    "        if len(valid_post_ids) == len(set(post_ids)) and len(valid_user_ids) == len(set(user_ids)):\n",
    "            # All UserIds are valid, proceed with bulk insertion\n",
    "            insert_chunk_into_db(query, chunk_data)\n",
    "        else:\n",
    "            # Some UserIds are invalid, filter out invalid entries\n",
    "#             valid_chunk_data = remove_invalid_entries(chunk_data, valid_post_ids)\n",
    "            valid_chunk_data = remove_invalid_entries_links(chunk_data, valid_post_ids, -5)\n",
    "            valid_chunk_data = remove_invalid_entries_links(valid_chunk_data, valid_user_ids, -1)\n",
    "            insert_chunk_into_db(query, valid_chunk_data)\n",
    "\n",
    "    print(f\"Processed {chunk_count} chunks of {chunk_size} elements each.\")\n",
    "\n",
    "for file in [\"PostHistory\"]:\n",
    "    input_file = \"askubuntu/\" + file + \".xml\"  # Path to your large XML file\n",
    "    post_hist_db_chunks(input_file, post_history_query, max_chunks=1)\n",
    "    \n",
    "    \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a976dd9",
   "metadata": {},
   "outputs": [],
   "source": [
    "#### INSERT INTO USERS\n",
    "exec_sql_file('create_schema.sql')\n",
    "\n",
    "def votes_db_chunks(input_file, query, max_chunks=1):\n",
    "    \"\"\"\n",
    "    Process large XML file in chunks of elements and stop after a certain number of chunks.\n",
    "    \n",
    "    :param input_file: Path to the large XML file.\n",
    "    :param chunk_size: Number of elements to process in each chunk.\n",
    "    :param max_chunks: Maximum number of chunks to process. MUST REMOVE\n",
    "    \"\"\"\n",
    "    \n",
    "    context = ET.iterparse(input_file, events=(\"start\", \"end\"))\n",
    "    \n",
    "    # Skip the root element but start processing children directly\n",
    "    # _, root = next(context)  # This gets the root but doesn't necessarily process it\n",
    "    \n",
    "    chunk_count = 0\n",
    "    elements_in_chunk = 0\n",
    "    chunk_data = []  # List to hold data to be inserted into the database\n",
    "\n",
    "    for event, elem in context:\n",
    "        if elem.tag == 'row':\n",
    "            # Collect data from element attributes, handle missing attributes\n",
    "            if elem.get('Id') is not None and elem.get('PostId') is not None:\n",
    "                element_data = (\n",
    "                    elem.get('Id'),elem.get('BountyAmount'),\n",
    "                    elem.get('CreationDate'),elem.get('PostId'),\n",
    "                    elem.get('UserId'),elem.get('VoteTypeId')\n",
    "                )\n",
    "                chunk_data.append(element_data)\n",
    "                elements_in_chunk += 1\n",
    "\n",
    "            # If we've reached the chunk size, save the chunk and reset\n",
    "            if elements_in_chunk >= chunk_size:\n",
    "                chunk_count += 1\n",
    "                post_ids = extract_ids_from_chunk(chunk_data, -3)\n",
    "                valid_post_ids = check_valid_fk_ids('posts', post_ids)\n",
    "                \n",
    "                if len(valid_post_ids) == len(set(post_ids)):\n",
    "                    # All UserIds are valid, proceed with bulk insertion\n",
    "                    insert_chunk_into_db(query, chunk_data)\n",
    "                    chunk_data = []  # Reset chunk data\n",
    "                    elements_in_chunk = 0\n",
    "                else:\n",
    "                    # Some UserIds are invalid, filter out invalid entries\n",
    "#                     valid_chunk_data = remove_invalid_entries(chunk_data, valid_post_ids, -2)\n",
    "                    valid_chunk_data = remove_invalid_entries_links(chunk_data, valid_post_ids, -3)\n",
    "                    insert_chunk_into_db(query, valid_chunk_data)\n",
    "                    chunk_data = []  # Reset chunk data\n",
    "                    elements_in_chunk = 0\n",
    "\n",
    "            elem.clear()\n",
    "\n",
    "            # Stop after processing the max number of chunks\n",
    "            # if chunk_count >= max_chunks:\n",
    "            #     print(f\"Reached maximum chunk count ({max_chunks}). Terminating.\")\n",
    "            #     break\n",
    "\n",
    "    # If there are any remaining elements in the final chunk, save it\n",
    "    if elements_in_chunk > 0: # and chunk_count < max_chunks:\n",
    "        chunk_count += 1\n",
    "        post_ids = extract_ids_from_chunk(chunk_data, -3)\n",
    "        valid_post_ids = check_valid_fk_ids('posts', post_ids)\n",
    "        if len(valid_post_ids) == len(set(post_ids)):\n",
    "            # All UserIds are valid, proceed with bulk insertion\n",
    "            insert_chunk_into_db(query, chunk_data)\n",
    "        else:\n",
    "            # Some UserIds are invalid, filter out invalid entries\n",
    "#             valid_chunk_data = remove_invalid_entries(chunk_data, valid_post_ids)\n",
    "            valid_chunk_data = remove_invalid_entries_links(chunk_data, valid_post_ids, -3)\n",
    "            insert_chunk_into_db(query, valid_chunk_data)\n",
    "\n",
    "    print(f\"Processed {chunk_count} chunks of {chunk_size} elements each.\")\n",
    "\n",
    "for file in [\"Votes\"]:\n",
    "    chunk_size = 5\n",
    "    input_file = \"askubuntu/\" + file + \".xml\"  # Path to your large XML file\n",
    "    votes_db_chunks(input_file, votes_query, max_chunks=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "c37f00e9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Inserted 2443 records into the database.\n",
      "Processed 1 chunks of 5000 elements each.\n"
     ]
    }
   ],
   "source": [
    "#### INSERT INTO USERS\n",
    "exec_sql_file('create_schema.sql')\n",
    "\n",
    "def tags_db_chunks(input_file, query, max_chunks=1):\n",
    "    \"\"\"\n",
    "    Process large XML file in chunks of elements and stop after a certain number of chunks.\n",
    "    \n",
    "    :param input_file: Path to the large XML file.\n",
    "    :param chunk_size: Number of elements to process in each chunk.\n",
    "    :param max_chunks: Maximum number of chunks to process. MUST REMOVE\n",
    "    \"\"\"\n",
    "    \n",
    "    context = ET.iterparse(input_file, events=(\"start\", \"end\"))\n",
    "    \n",
    "    # Skip the root element but start processing children directly\n",
    "    # _, root = next(context)  # This gets the root but doesn't necessarily process it\n",
    "    \n",
    "    chunk_count = 0\n",
    "    elements_in_chunk = 0\n",
    "    chunk_data = []  # List to hold data to be inserted into the database\n",
    "\n",
    "    for event, elem in context:\n",
    "        if elem.tag == 'row':\n",
    "            # Collect data from element attributes, handle missing attributes\n",
    "            if elem.get('Id') is not None and elem.get('ExcerptPostId') is not None and elem.get('WikiPostId'):\n",
    "                element_data = (\n",
    "                    elem.get('Id'),elem.get('Count'),\n",
    "                    elem.get('ExcerptPostId'),\n",
    "                    elem.get('TagName'),\n",
    "                    elem.get('WikiPostId')\n",
    "                )\n",
    "                chunk_data.append(element_data)\n",
    "                elements_in_chunk += 1\n",
    "\n",
    "            # If we've reached the chunk size, save the chunk and reset\n",
    "            if elements_in_chunk >= chunk_size:\n",
    "                chunk_count += 1\n",
    "                post_ids = extract_ids_from_chunk(chunk_data, -3)\n",
    "                valid_post_ids = check_valid_fk_ids('posts', post_ids)\n",
    "                wiki_ids = extract_ids_from_chunk(chunk_data, -1)\n",
    "                valid_wiki_ids = check_valid_fk_ids('posts', wiki_ids)\n",
    "                \n",
    "                if len(valid_post_ids) == len(set(post_ids)) and len(wiki_ids) == len(set(valid_wiki_ids)):\n",
    "                    # All UserIds are valid, proceed with bulk insertion\n",
    "                    insert_chunk_into_db(query, chunk_data)\n",
    "                    chunk_data = []  # Reset chunk data\n",
    "                    elements_in_chunk = 0\n",
    "                else:\n",
    "                    # Some UserIds are invalid, filter out invalid entries\n",
    "#                     valid_chunk_data = remove_invalid_entries(chunk_data, valid_post_ids, -2)\n",
    "                    valid_chunk_data = remove_invalid_entries_links(chunk_data, valid_post_ids, -3)\n",
    "                    valid_chunk_data = remove_invalid_entries_links(valid_chunk_data, valid_wiki_ids, -1)\n",
    "                    insert_chunk_into_db(query, valid_chunk_data)\n",
    "                    chunk_data = []  # Reset chunk data\n",
    "                    elements_in_chunk = 0\n",
    "\n",
    "            elem.clear()\n",
    "\n",
    "            # Stop after processing the max number of chunks\n",
    "#             if chunk_count >= max_chunks:\n",
    "#                 print(f\"Reached maximum chunk count ({max_chunks}). Terminating.\")\n",
    "#                 break\n",
    "\n",
    "    # If there are any remaining elements in the final chunk, save it\n",
    "    if elements_in_chunk > 0: # and chunk_count < max_chunks:\n",
    "        chunk_count += 1\n",
    "        post_ids = extract_ids_from_chunk(chunk_data, -3)\n",
    "        valid_post_ids = check_valid_fk_ids('posts', post_ids)\n",
    "        wiki_ids = extract_ids_from_chunk(chunk_data, -1)\n",
    "        valid_wiki_ids = check_valid_fk_ids('posts', wiki_ids)\n",
    "        if len(valid_post_ids) == len(set(post_ids)) and len(valid_wiki_ids) == len(set(wiki_ids)):\n",
    "            # All UserIds are valid, proceed with bulk insertion\n",
    "            insert_chunk_into_db(query, chunk_data)\n",
    "        else:\n",
    "            # Some UserIds are invalid, filter out invalid entries\n",
    "#             valid_chunk_data = remove_invalid_entries(chunk_data, valid_post_ids)\n",
    "            valid_chunk_data = remove_invalid_entries_links(chunk_data, valid_post_ids, -3)\n",
    "            valid_chunk_data = remove_invalid_entries_links(valid_chunk_data, valid_wiki_ids, -1)\n",
    "            insert_chunk_into_db(query, valid_chunk_data)\n",
    "    \n",
    "    print(f\"Processed {chunk_count} chunks of {chunk_size} elements each.\")\n",
    "\n",
    "for file in [\"Tags\"]:\n",
    "    chunk_size = 5000\n",
    "    input_file = \"askubuntu/\" + file + \".xml\"  # Path to your large XML file\n",
    "    tags_db_chunks(input_file, tags_query, max_chunks=1)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f04bbc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Items not in array1: 183978\n",
      "Time taken: 0.218802 seconds\n"
     ]
    },
    {
     "ename": "",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31mThe Kernel crashed while executing code in the current cell or a previous cell. \n",
      "\u001b[1;31mPlease review the code in the cell(s) to identify a possible cause of the failure. \n",
      "\u001b[1;31mClick <a href='https://aka.ms/vscodeJupyterKernelCrash'>here</a> for more info. \n",
      "\u001b[1;31mView Jupyter <a href='command:jupyter.viewOutput'>log</a> for further details."
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import time\n",
    "\n",
    "# Generate example arrays\n",
    "array1 = np.random.randint(0, 1000000, size=1000000).tolist()  # Array of 1 million items\n",
    "array2 = np.random.randint(0, 1000000, size=500000).tolist()    # Array of half a million items\n",
    "\n",
    "# Start timing\n",
    "start_time = time.time()\n",
    "\n",
    "# Convert array1 to a set for faster lookup\n",
    "set1 = set(array1)\n",
    "\n",
    "# Find items in array2 that are not in array1\n",
    "result = [item for item in array2 if item not in set1]\n",
    "\n",
    "# End timing\n",
    "end_time = time.time()\n",
    "\n",
    "# Calculate elapsed time\n",
    "elapsed_time = end_time - start_time\n",
    "\n",
    "print(f\"Items not in array1: {len(result)}\")\n",
    "print(f\"Time taken: {elapsed_time:.6f} seconds\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a7cc42d",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d86bed75",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ca59395",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
